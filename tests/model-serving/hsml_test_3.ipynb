{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd68d77c",
   "metadata": {},
   "source": [
    "# Hopsworks Machine Learning (HSML) - Quickstart Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf26bd",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606eed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading Faker-12.3.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages (from python-dateutil>=2.4->faker) (1.15.0)\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-12.3.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaceddc3",
   "metadata": {},
   "source": [
    "## Generate dataset and train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283eb9d8",
   "metadata": {},
   "source": [
    "### Generate dataset for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aac02fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Credit to https://github.com/minimaxir/ml-data-generator\n",
    "\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Utils\n",
    "\n",
    "cat1_dict = { 1: 2, 2: -1, 3: 4, 4: 6, 5: -6, 6: 4, 7: 9, 8: 10, 9: -4, 10: 0 }\n",
    "cat2_dict = { 'a': 1, 'b': 4, 'c': 15 }\n",
    "\n",
    "def text1_transform(text):\n",
    "    text_split = text.split(\" \")\n",
    "    num_words = len(text_split)\n",
    "\n",
    "    total = 0\n",
    "    for i, word in enumerate(text_split):\n",
    "        total += np.power(len(word), 1 + i / 100) / num_words\n",
    "\n",
    "    return total\n",
    "\n",
    "def text2_transform(text):\n",
    "    counts = Counter(text)\n",
    "    return sum([counts.get(x, 0) for x in 'aeiou'])\n",
    "\n",
    "def cat1_transform(key):\n",
    "    return cat1_dict[key]\n",
    "\n",
    "def cat2_transform(key):\n",
    "    return cat2_dict[key]\n",
    "\n",
    "def datetime2_transform(datetime_col):\n",
    "    hour = datetime_col.dt.hour\n",
    "    dayofweek = datetime_col.dt.dayofweek\n",
    "    year = datetime_col.dt.year\n",
    "\n",
    "    hour_tf = np.array([0.1 if x <= 6 or x >= 20 else 0.3 for x in hour])\n",
    "    dayofweek_tf = np.array([0.2 if x >= 5 else 0.5 for x in dayofweek])\n",
    "    year_tf = np.array([0.2 if x == 2017 else 0.5 for x in year])\n",
    "\n",
    "    return hour_tf + dayofweek_tf + year_tf\n",
    "\n",
    "# Generate dataset\n",
    "\n",
    "fake = Faker()\n",
    "nrow = 100\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Dummy data\n",
    "\n",
    "df['id'] = range(1, nrow+1)\n",
    "df['name'] = [fake.name()\n",
    "              for _ in range(nrow)]\n",
    "\n",
    "# Numeric data\n",
    "\n",
    "df['num1'] = np.random.normal(0, 1, nrow)\n",
    "df['num2'] = np.random.randint(1, 100, nrow)\n",
    "\n",
    "# Text data\n",
    "\n",
    "df['text1'] = [fake.sentence(nb_words=10, variable_nb_words=True)\n",
    "               for _ in range(nrow)]\n",
    "df['text2'] = [fake.sentence(nb_words=4, variable_nb_words=True)\n",
    "               for _ in range(nrow)]\n",
    "# Categorical data\n",
    "\n",
    "df['cat1'] = np.random.randint(1, 10, nrow)\n",
    "df['cat2'] = np.random.choice(['a', 'b', 'c'], nrow, p=[0.5, 0.45, 0.05])\n",
    "\n",
    "# Datetime data\n",
    "\n",
    "df['datetime1'] = [fake.date_time_between(\n",
    "    start_date=datetime(2017, 1, 1),\n",
    "    end_date=datetime(2018, 12, 31))\n",
    "    for _ in range(nrow)]\n",
    "df['datetime2'] = df['datetime1'].apply(lambda x: x + pd.Timedelta(seconds=np.random.randint(0, 72 * 60 * 60)))\n",
    "\n",
    "# Process each generated field to derive a target statistic.\n",
    "\n",
    "num1_tf = df['num1']**2\n",
    "num2_tf = df['num2'] / 10 * num1_tf\n",
    "\n",
    "text1_tf = df['text1'].apply(text1_transform)\n",
    "text2_tf = df['text2'].apply(text2_transform)\n",
    "\n",
    "datetime1_tf = (df['datetime2'] - df['datetime1'])\n",
    "datetime1_tf = datetime1_tf / np.timedelta64(1, 'h') / 12\n",
    "datetime2_tf = datetime2_transform(df['datetime2'])\n",
    "\n",
    "cat1_tf = df['cat1'].apply(cat1_transform)\n",
    "cat2_tf = df['cat2'].apply(cat2_transform)\n",
    "\n",
    "df['target'] = (np.log10(num1_tf + num2_tf) +\n",
    "                np.power(text1_tf * text2_tf, datetime2_tf) +\n",
    "                datetime1_tf + np.max(cat1_tf + cat2_tf))\n",
    "\n",
    "bins = df['target'].quantile([0.0, 0.5, 1.0])\n",
    "df['target'] = pd.cut(df['target'], bins=bins, labels=False, include_lowest=True).astype(int)\n",
    "#df_copy.to_csv(\"gen_df_binary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac113d8",
   "metadata": {},
   "source": [
    "### Train a dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f120ad79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dummy_classifier.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "X = df\n",
    "y = np.random.randint(low=0, high=2, size=100)\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X, y)\n",
    "\n",
    "acc = dummy_clf.score(X, y)\n",
    "\n",
    "joblib.dump(dummy_clf, \"dummy_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c186e8",
   "metadata": {},
   "source": [
    "## Register and serve a model with HSML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5bda5e",
   "metadata": {},
   "source": [
    "### Create a connection with Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058ed491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "connection = hsml.connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1a95e",
   "metadata": {},
   "source": [
    "### Register the classifier in the Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9b3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get connection with the Model Registry\n",
    "mr = connection.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca5416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import hdfs\n",
    "hdfs.mkdir(\"/Resources/dummy_classifier\")\n",
    "hdfs.copy_to_hdfs(\"dummy_classifier.pkl\", \"/Resources/dummy_classifier\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "521728c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dba908d5ee4a14b8aaeddcdf198a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exported model dummy_classifier with version 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hsml.python.model.Model at 0x7f627bb51d00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "model_schema = ModelSchema(input_schema=Schema(X), output_schema=Schema(y))\n",
    "\n",
    "dummy_model = mr.python.create_model(\"dummy_classifier\", description=\"Dummy binary classifier\", metrics={'accuracy': acc}, model_schema=model_schema, input_example=X)\n",
    "#dummy_model.save(\"dummy_classifier.pkl\")\n",
    "dummy_model.save(\"/Resources/dummy_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73021dc2",
   "metadata": {},
   "source": [
    "### Deploy the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8ccd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get connection with Model Serving\n",
    "ms = connection.get_model_serving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b1c5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = mr.get_best_model(\"dummy_classifier\", \"accuracy\", \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployment\n",
    "dummy_model.deploy(name=\"dummyclassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83906e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_predictor = ms.create_predictor(dummy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_predictor.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_predictor.update_from_response_json(dummy_predictor.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132e0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd16ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classifier = ms.create_deployment(dummy_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c52d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classifier.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}